{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03a768d9",
   "metadata": {},
   "source": [
    "<p>Содержание:</p>\n",
    "<p>1  Описание работы</p>\n",
    "<p>2  Загрузка необходимых библиотек</p>\n",
    "<p>3  Загрузка, анализ и предобработка данных</p>\n",
    "<p>3.1  Загрузка excel данных</p>\n",
    "<p>3.2  Обработка целевых меток</p>\n",
    "<p>3.3  Обработка дубликатов</p>\n",
    "<p>3.4  Загрузка субтитров и словаря</p>\n",
    "<p>3.5  Обработка пропусков</p>\n",
    "<p>3.6  Оценка дисбаланса классов</p>\n",
    "<p>4  Препроцессинг данных</p>\n",
    "<p>4.1  Разбивка данных на обучающую и тестовую выборки</p>\n",
    "<p>4.2  Векторизация</p>\n",
    "<p>5  Обучение модели MultinomialNB</p>\n",
    "<p>5.1  Выбор метрики качества</p>\n",
    "<p>5.2  Построение pipiline и обучение</p>\n",
    "<p>5.3  Оценка модели на тестовой (отложенной) выборке</p>\n",
    "<p>5.4  Выводы по результатам работы модели</p>\n",
    "<p>5.5  Обучение модели MultinomialNB на всех данных. Сохранение модели.</p>\n",
    "<p>6  Выводы и рекомендации</p>\n",
    "<p>Описание работы</p>\n",
    "<p>Проект:</p>\n",
    "<p>Запрос сформирован тем, что просмотр фильмов на оригинальном языке - это популярный и действенный метод упражнений по изучению иностранных языков. Важно выбрать фильм, который подходит студенту по уровню сложности, т.е. студент понимал 50-70 % диалогов. Чтобы выполнить это условие, преподаватель должен посмотреть фильм и решить, какому уровню он соответствует. Однако это требует больших временных затрат.</p>\n",
    "\n",
    "<p>Заказчику необходимо:\n",
    "\n",
    "<p> Разработать ML решение для автоматического определения уровня сложности англоязычных фильмов. За время работы над проектом вы обучите языковую модель, разработаете для неё веб-интерфейс и создадите микросервис. </p>\n",
    "\n",
    "<p>Исходные данные:\n",
    "\n",
    "<p>размеченный датасет с названиями фильмов в формате excel, c субтитрами и меткой уровня сложности языка (A2/B1/B2/C1/C2);</p>\n",
    "<p>файлы субтитров в формате .srt, отсортированные по каталогам в соответствии с уровнем сложности. Предварительно все субтитры были перенесены в общую папку для 'D:/English_films_level/Datasets/Subtitles загрузки';</p>\n",
    "<p>словари Oxford (на 3000 и 5000 тыс.слов), в которых слова на английском сгруппированны по уровню сложности.</p>\n",
    "<p>План работы:</p>\n",
    "\n",
    "<p>загрузка необходимых библиотек;</p>\n",
    "<p>загрузка и ознакомление с данными;</p>\n",
    "<p>предобработка данных (очистка от дубликатов, проверка наличия разметки для обучающих данных, определение исходного количества представленных данных, очитска текста субтитров, разбивка на обучающую и тестовую выборки);</p>\n",
    "<p>препроцессинг данных (преобразование данных для обучения модели);</p>\n",
    "<p>определение метрики качества;</p>\n",
    "<p>обучение модели с побором гиперпараметров;</p>\n",
    "<p>оценка модели на тестовой выборке;</p>\n",
    "<p>рекомендации к улучшению проекта.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb90652b",
   "metadata": {},
   "source": [
    "## Загрузка необходимых библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a124c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pysrt in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (1.1.2)\n",
      "Requirement already satisfied: chardet in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from pysrt) (4.0.0)\n",
      "Requirement already satisfied: spacy in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (3.5.3)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from spacy) (2.28.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from spacy) (22.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from spacy) (2.4.6)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from spacy) (0.7.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from spacy) (65.6.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from spacy) (1.10.8)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from spacy) (1.1.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from spacy) (1.23.5)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from spacy) (5.2.1)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from spacy) (1.0.4)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from spacy) (4.64.1)\n",
      "Requirement already satisfied: pathy>=0.10.0 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from spacy) (0.10.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from spacy) (1.0.9)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from spacy) (8.1.10)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from spacy) (3.0.8)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from spacy) (2.0.7)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.14)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.12.7)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from typer<0.8.0,>=0.3.0->spacy) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from jinja2->spacy) (2.1.1)\n",
      "Requirement already satisfied: en_core_web_sm in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (3.5.0)\n",
      "Requirement already satisfied: spacy<3.6.0,>=3.5.0 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from en_core_web_sm) (3.5.3)\n",
      "Requirement already satisfied: pathy>=0.10.0 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en_core_web_sm) (0.10.1)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en_core_web_sm) (0.7.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en_core_web_sm) (2.28.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en_core_web_sm) (2.0.7)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en_core_web_sm) (3.3.0)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en_core_web_sm) (1.0.4)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en_core_web_sm) (3.1.2)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en_core_web_sm) (5.2.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en_core_web_sm) (22.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en_core_web_sm) (3.0.12)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en_core_web_sm) (1.23.5)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en_core_web_sm) (2.4.6)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en_core_web_sm) (4.64.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en_core_web_sm) (65.6.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en_core_web_sm) (1.1.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en_core_web_sm) (3.0.8)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en_core_web_sm) (8.1.10)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en_core_web_sm) (2.0.8)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en_core_web_sm) (1.0.9)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en_core_web_sm) (1.10.8)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en_core_web_sm) (4.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en_core_web_sm) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en_core_web_sm) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en_core_web_sm) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en_core_web_sm) (2.0.4)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en_core_web_sm) (0.0.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en_core_web_sm) (0.7.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.6.0,>=3.5.0->en_core_web_sm) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en_core_web_sm) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from jinja2->spacy<3.6.0,>=3.5.0->en_core_web_sm) (2.1.1)\n",
      "Collecting en-core-web-sm==3.5.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 12.8/12.8 MB 6.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.6.0,>=3.5.0 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from en-core-web-sm==3.5.0) (3.5.3)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.4)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.10)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.64.1)\n",
      "Requirement already satisfied: pathy>=0.10.0 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.10.1)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (5.2.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.7)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.10.8)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.1.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.23.5)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.4.6)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.9)\n",
      "Requirement already satisfied: setuptools in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (65.6.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.28.1)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.12)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (22.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.3.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.8)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.1.1)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2022.12.7)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\lariosik\\anaconda3\\lib\\site-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.1.1)\n",
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "# установим недостающие пакеты\n",
    "!pip install pysrt\n",
    "!pip install spacy\n",
    "!pip install en_core_web_sm\n",
    "!python -m spacy download en_core_web_sm\n",
    "\n",
    "# импортируем общие библиотеки\n",
    "import os\n",
    "import numpy as np               \n",
    "import pandas as pd              \n",
    "import pysrt                     \n",
    "import spacy                     \n",
    "import re\n",
    "import warnings \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# импортируем библиотеки sklearn \n",
    "from sklearn.datasets import load_files  \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report, f1_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.compose import ColumnTransformer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a3cad0",
   "metadata": {},
   "source": [
    "Загрузка excel данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "580df571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# сразу зададим константу случайного состояния для повторяемости результатов  \n",
    "RAND_ST = 777"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cff6e555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del df\n",
    "# загрузим данные из excel таблицы\n",
    "df = pd.read_excel('D:/English_films_level/Datasets/movies_labels.xlsx', index_col='id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c3e681d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Movie</th>\n",
       "      <th>Level</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10_Cloverfield_lane(2016)</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10_things_I_hate_about_you(1999)</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A_knights_tale(2001)</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A_star_is_born(2018)</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Aladdin(1992)</td>\n",
       "      <td>A2/A2+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>All_dogs_go_to_heaven(1989)</td>\n",
       "      <td>A2/A2+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>An_American_tail(1986)</td>\n",
       "      <td>A2/A2+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Babe(1995)</td>\n",
       "      <td>A2/A2+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Back_to_the_future(1985)</td>\n",
       "      <td>A2/A2+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Banking_On_Bitcoin(2016)</td>\n",
       "      <td>C1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Movie   Level\n",
       "id                                          \n",
       "0          10_Cloverfield_lane(2016)      B1\n",
       "1   10_things_I_hate_about_you(1999)      B1\n",
       "2               A_knights_tale(2001)      B2\n",
       "3               A_star_is_born(2018)      B2\n",
       "4                      Aladdin(1992)  A2/A2+\n",
       "5        All_dogs_go_to_heaven(1989)  A2/A2+\n",
       "6             An_American_tail(1986)  A2/A2+\n",
       "7                         Babe(1995)  A2/A2+\n",
       "8           Back_to_the_future(1985)  A2/A2+\n",
       "9           Banking_On_Bitcoin(2016)      C1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59f8a435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 241 entries, 0 to 240\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Movie   241 non-null    object\n",
      " 1   Level   241 non-null    object\n",
      "dtypes: object(2)\n",
      "memory usage: 5.6+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9e6100",
   "metadata": {},
   "source": [
    "Таблица excel содержит 241 размеченных фильмов/сериалов."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934c3ad0",
   "metadata": {},
   "source": [
    "## Обработка целевых меток"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 861,
   "id": "bba97fec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['B1', 'B2', 'A2/A2+', 'C1', 'B1, B2', 'A2/A2+, B1', 'A2'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 861,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# определим какие целевые метки содержат данные \n",
    "df['Level'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2959d3f3",
   "metadata": {},
   "source": [
    "В данных присутствуют множественные метки для одного и того же фильма (например 'A2/A2+'). Зададим для таких фильмов единственную метку, соответсвующую максимальному значению уровня сложности.\n",
    "\n",
    "Закодируем метки числовыми пордяками от 1 до 4.\n",
    "\n",
    "Так же отмечено отсутствие фильмов с меткой A1, соответствующих самому легкому уровню сложности. После знакомства с представленными фильмами можно заметить, что таким фильмам (точнее мультикам) присвоена метка A2 (например для мультика 'Toy_story(1995)' присвоена метка A2, несмотря на то что многими специалистами уровень сложности определен как A1). В итоге предлагается придерживаться исходных размеченных данных, представленных Заказчиком и исключить из предсказания метку A1 и считать что она объединена с меткой A2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d77031e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# определим словарь меток с кодированием значений\n",
    "label_dict = {'A2': 1,\n",
    "              'A2/A2+': 1,\n",
    "              'B1': 2,\n",
    "              'A2/A2+, B1': 2,\n",
    "              'B2': 3,\n",
    "              'B1, B2': 3,\n",
    "              'C1': 4}\n",
    "# заменим метки числовыми значениями\n",
    "df = df.replace(label_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6252ea28",
   "metadata": {},
   "source": [
    "## Обработка дубликатов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5e3f53f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Movie</th>\n",
       "      <th>Level</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Powder(1995)</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Inside_out(2015)</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Inside_out(2015)</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>Powder(1995)</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>The_terminal(2004)</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>The_terminal(2004)</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Movie  Level\n",
       "id                           \n",
       "38        Powder(1995)      2\n",
       "43    Inside_out(2015)      2\n",
       "44    Inside_out(2015)      2\n",
       "68        Powder(1995)      2\n",
       "83  The_terminal(2004)      2\n",
       "99  The_terminal(2004)      2"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# проверим данные на наличие дубликатов\n",
    "df[df.duplicated(keep=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b760554",
   "metadata": {},
   "source": [
    "В данных присутствует 3 дубликата. Удалим их."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f82b726",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(238, 2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# удалим дубликаты \n",
    "df = df.drop_duplicates()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4847e3f",
   "metadata": {},
   "source": [
    "## Загрузка субтитров и словаря"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fa94b1",
   "metadata": {},
   "source": [
    "Далее загрузим субтитры к фильмам из папки 'D:\\English_films_level\\Datasets\\Subtitles'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b13ef1ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество названий фильмов в папке с субтитрами: 115\n"
     ]
    }
   ],
   "source": [
    "# загрузим в список имена файлов из папки с субтитрами\n",
    "films_name = os.listdir(path= 'D:/English_films_level/Datasets/Subtitles/')\n",
    "# определим количество названий фильмов\n",
    "print(f'Количество названий фильмов в папке с субтитрами: {len(films_name)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf7eb63a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество фильмов, имеющих метку и субтитры: 106\n"
     ]
    }
   ],
   "source": [
    "# проверим для скольких фильмов, имеющих метку из таблицы, предоставлены субтитры \n",
    "films_filtr = set(films_name) & set(df['Movie'] + '.srt')\n",
    "print(f'Количество фильмов, имеющих метку и субтитры: {len(films_filtr)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f78b73",
   "metadata": {},
   "source": [
    "Далее загрузка субтитров в датафрейм будет осуществлена с применением функции очистки текста. Так же будет загружен словарь Classic Oxford для дальнейшего подсчета количества уникальных лемм, содержащихся в каждом тексте субтитров согласно уровню сложности."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7504f851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del american_oxford\n",
    "oxford = load_files('D:/English_films_level/Datasets/Oxford/Classic Oxford/', shuffle=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2302fbc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a, an \\r\\nabout \\r\\nabove \\r\\nacross'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oxford.data[0][:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7301bbf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Movie</th>\n",
       "      <th>Level</th>\n",
       "      <th>subs</th>\n",
       "      <th>A1_lemma_cnt</th>\n",
       "      <th>A2_lemma_cnt</th>\n",
       "      <th>B1_lemma_cnt</th>\n",
       "      <th>B2_lemma_cnt</th>\n",
       "      <th>C1_lemma_cnt</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10_Cloverfield_lane(2016)</td>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10_things_I_hate_about_you(1999)</td>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A_knights_tale(2001)</td>\n",
       "      <td>3</td>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A_star_is_born(2018)</td>\n",
       "      <td>3</td>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Aladdin(1992)</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>All_dogs_go_to_heaven(1989)</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>An_American_tail(1986)</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Babe(1995)</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Back_to_the_future(1985)</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Banking_On_Bitcoin(2016)</td>\n",
       "      <td>4</td>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Movie  Level subs  A1_lemma_cnt  A2_lemma_cnt  \\\n",
       "id                                                                             \n",
       "0          10_Cloverfield_lane(2016)      2                0.0           0.0   \n",
       "1   10_things_I_hate_about_you(1999)      2                0.0           0.0   \n",
       "2               A_knights_tale(2001)      3                0.0           0.0   \n",
       "3               A_star_is_born(2018)      3                0.0           0.0   \n",
       "4                      Aladdin(1992)      1                0.0           0.0   \n",
       "5        All_dogs_go_to_heaven(1989)      1                0.0           0.0   \n",
       "6             An_American_tail(1986)      1                0.0           0.0   \n",
       "7                         Babe(1995)      1                0.0           0.0   \n",
       "8           Back_to_the_future(1985)      1                0.0           0.0   \n",
       "9           Banking_On_Bitcoin(2016)      4                0.0           0.0   \n",
       "\n",
       "    B1_lemma_cnt  B2_lemma_cnt  C1_lemma_cnt  \n",
       "id                                            \n",
       "0            0.0           0.0           0.0  \n",
       "1            0.0           0.0           0.0  \n",
       "2            0.0           0.0           0.0  \n",
       "3            0.0           0.0           0.0  \n",
       "4            0.0           0.0           0.0  \n",
       "5            0.0           0.0           0.0  \n",
       "6            0.0           0.0           0.0  \n",
       "7            0.0           0.0           0.0  \n",
       "8            0.0           0.0           0.0  \n",
       "9            0.0           0.0           0.0  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# зададим регулярнеы выражения для очистки текста\n",
    "HTML = re.compile(r'<.*?>') # html тэги меняем на пробел\n",
    "TAG = r'{.*?}' # тэги меняем на пробел\n",
    "COMMENTS = r'[\\(\\[][A-Za-z ]+[\\)\\]]' # комменты в скобках меняем на пробел\n",
    "LETTERS = r'[^a-zA-Z\\.,!? ]' # все что не буквы меняем на пробел \n",
    "SPACES = r'([ ])\\1+' # повторяющиеся пробелы меняем на один пробел\n",
    "DOTS = r'[\\.]+' # многоточие меняем на точку\n",
    "# SYMB = r\"[^\\w\\d'\\s]\" # знаки препинания кроме апострофа\n",
    "\n",
    "# напишем функцию для очистки субтитров\n",
    "def clean_subs(subs):\n",
    "    subs = subs[1:] # удаляем первый рекламный субтитр\n",
    "    txt = re.sub(HTML, ' ', subs.text) # html тэги меняем на пробел\n",
    "    txt = re.sub(COMMENTS, ' ', txt) # комменты в скобках меняем на пробел\n",
    "    txt = re.sub(LETTERS, ' ', txt) # все что не буквы меняем на пробел\n",
    "    txt = re.sub(DOTS, r'.', txt) # многоточие меняем на точку\n",
    "    txt = re.sub(SPACES, r'\\1', txt) # повторяющиеся пробелы меняем на один пробел\n",
    "    # txt = re.sub(SYMB, '', txt) # знаки препинания кроме апострофа на пустую строку\n",
    "    txt = re.sub('www', '', txt) # кое-где остаётся www, то же меняем на пустую строку\n",
    "    txt = txt.lstrip() # обрезка пробелов слева\n",
    "    txt = txt.encode('ascii', 'ignore').decode() # удаляем все что не ascii символы   \n",
    "    txt = txt.lower() # текст в нижний регистр\n",
    "    return txt\n",
    "\n",
    "\n",
    "# функция возвращающая количество уникальных лемм\n",
    "# в тексте субтитров по каждому уровню\n",
    "def lemma_count(lemmas, oxf, cat):\n",
    "    func_dict = {'A1': 0,\n",
    "                 'A2': 1,\n",
    "                 'B1': 2,\n",
    "                 'B2': 3,\n",
    "                 'C1': 4}\n",
    "    level = func_dict[cat]\n",
    "    oxf_word_list = oxf[level].split()\n",
    "    words = [lemma for lemma in lemmas if lemma in oxf_word_list]\n",
    "\n",
    "    return len(set(words))\n",
    "\n",
    "# загрузим для каждого фильма субтитры с использованием библиотеки pysrt\n",
    "for film in films_filtr:\n",
    "    try: \n",
    "        subs = pysrt.open(f'D:/English_films_level/Datasets/Subtitles/{film}')\n",
    "        \n",
    "    except:\n",
    "        subs = pysrt.open(f'D:/English_films_level/Datasets/Subtitles/{film}', encoding='iso-8859-1')\n",
    " \n",
    "    # вызов функии для очистки текста\n",
    "    cln_subs = clean_subs(subs)\n",
    "    df.loc[df['Movie'] == film[:-4], 'subs'] = cln_subs\n",
    "    \n",
    "    # используем библиотеку spacy для лемматизации \n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    doc = nlp(cln_subs)\n",
    "    lemma_list = [token.lemma_ for token in doc]\n",
    "    \n",
    "    # в цикле по каждой метке запишем в датафрейм кол-во уникальных лемм\n",
    "    for lvl in ['A1', 'A2', 'B1', 'B2', 'C1']:\n",
    "        df.loc[df['Movie'] == film[:-4], lvl+'_lemma_cnt'] = lemma_count(lemma_list, oxford.data, lvl)\n",
    "                 \n",
    "# выведем первые 10 записей\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac569bb5",
   "metadata": {},
   "source": [
    "В ячейке выше возникли проблемы с загрузкой subs (помощь чата не сработала), что отразится на отображении результатов ниже. Буду благодарна, если объясните, что не так помимо полного пути к файлам - короткие пути выдают ошибку поскольку работаю в http://localhost:8888/notebooks/work_notebook.ipynb#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57581ada",
   "metadata": {},
   "source": [
    "## Обработка пропусков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7b7a0f27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Movie</th>\n",
       "      <th>Level</th>\n",
       "      <th>subs</th>\n",
       "      <th>A1_lemma_cnt</th>\n",
       "      <th>A2_lemma_cnt</th>\n",
       "      <th>B1_lemma_cnt</th>\n",
       "      <th>B2_lemma_cnt</th>\n",
       "      <th>C1_lemma_cnt</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Movie, Level, subs, A1_lemma_cnt, A2_lemma_cnt, B1_lemma_cnt, B2_lemma_cnt, C1_lemma_cnt]\n",
       "Index: []"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# найдем пропуски в данных после загрузки\n",
    "df[df['subs'].isna()].sort_values('Movie')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "686d53a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# выдедем отсоритрованный список фильмов  для которых отсутствую субтитры в папке `'D:\\English_films_level\\Datasets\\Subtitles'`\n",
    "sorted(set(df['Movie'] + '.srt') - set(films_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dabda07",
   "metadata": {},
   "source": [
    "Фильмы, для которых в датафрейме отсутствуют субтитры, это те же фильмы, для которых действительно в папке отсутствуют файлы .srt (ошибка в загрузке данных исключается). Удалим их из датафрейма."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8422659c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Movie</th>\n",
       "      <th>Level</th>\n",
       "      <th>subs</th>\n",
       "      <th>A1_lemma_cnt</th>\n",
       "      <th>A2_lemma_cnt</th>\n",
       "      <th>B1_lemma_cnt</th>\n",
       "      <th>B2_lemma_cnt</th>\n",
       "      <th>C1_lemma_cnt</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Movie, Level, subs, A1_lemma_cnt, A2_lemma_cnt, B1_lemma_cnt, B2_lemma_cnt, C1_lemma_cnt]\n",
       "Index: []"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# удалим фильмы с пропущенными субтитрами\n",
    "df.dropna(inplace=True)\n",
    "# проверим удаление\n",
    "df[df['subs'].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb40dde7",
   "metadata": {},
   "source": [
    "## Оценка дисбаланса классов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776fb4cb",
   "metadata": {},
   "source": [
    "Для классификации и определения метрики качества важно проверять данные на наличие дисбаланса целевых классов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "788dee8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    39\n",
       "3    37\n",
       "1    25\n",
       "4     6\n",
       "Name: Level, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# определим количество фильмов, представленных в кажой категории сложности\n",
    "df['Level'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80430e56",
   "metadata": {},
   "source": [
    "В данных присутствует дисбаланс по целевым классам (меткам):\n",
    "\n",
    "37 фильмов с уровнем сложности 3 (метка B2);\n",
    "39 фильмов с уровнем сложности 2 (метка B1);\n",
    "6 фильмов с уровнем сложности 4 (метка C1);\n",
    "25 фильма с уровнем сложности 1 (метка A2).\n",
    "\n",
    "Вывод: для борьбы с дисбалансом классов есть несколько методов, которые не являются универсальными по своей сути и каждый должен определяться исходя из требований задачи и исходных данных. \n",
    "\n",
    "В данном случае было бы уместнее применить upsampling (выравнивание выборки за счет увеличения методом дублирования фильмов с миноритарными классами). Но с другой стороны увеличение выборки за счет дублирования текста в миноритарных классах не приведет к увеличению словаря модели и как следствие не увеличит качество прогнозов. В таком случае предлагается оставить выборку без изменений."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a90762d",
   "metadata": {},
   "source": [
    "## Препроцессинг данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26423633",
   "metadata": {},
   "source": [
    "## Разбивка данных на обучающую и тестовую выборки\n",
    "\n",
    "Разбивка данных будет произведена в соотношении обучающая/тестовая выборка - 80/20 %. Важным моментом который стоит учитывать - сохранение соотношения баланса классов в обеих выборках. Для этого будет использован параметр 'stratify'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fc71ceff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер обучающей выборки: ((85, 6), (85,))\n",
      "Размер тестовой выборки: ((22, 6), (22,))\n"
     ]
    }
   ],
   "source": [
    "# разобъем данные на валидационную и тестовую выборки\n",
    "df_train, df_test = train_test_split(df, random_state=RAND_ST, test_size=.2, stratify=df['Level'])\n",
    "# определим независимые признаки и зависимую целевую метку\n",
    "X_train = df_train.drop(['Level', 'Movie'], axis=1)\n",
    "y_train = df_train['Level']\n",
    "X_test = df_test.drop(['Level', 'Movie'], axis=1)\n",
    "y_test = df_test['Level']\n",
    "# определим размеры обучающей и тестовой выборки\n",
    "print(f'Размер обучающей выборки: {X_train.shape, y_train.shape}')\n",
    "print(f'Размер тестовой выборки: {X_test.shape, y_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe50f07",
   "metadata": {},
   "source": [
    "## Векторизация\n",
    "\n",
    "Векторизуем текст субтитров с помощью 'CountVectorizer' с гиперпараметром min_df=4, чтобы исключить специфические слова для определенных фильмов, которые не несут явной языковой нагрузки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f2eac76b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# векторизуем текст субтитров обучающей и тестовой выборки\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m vect \u001b[38;5;241m=\u001b[39m \u001b[43mCountVectorizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmin_df\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msubs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m X_train_vect \u001b[38;5;241m=\u001b[39m vect\u001b[38;5;241m.\u001b[39mtransform(X_train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msubs\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      4\u001b[0m X_test_vect \u001b[38;5;241m=\u001b[39m vect\u001b[38;5;241m.\u001b[39mtransform(X_test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msubs\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1338\u001b[0m, in \u001b[0;36mCountVectorizer.fit\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1322\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, raw_documents, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1323\u001b[0m     \u001b[38;5;124;03m\"\"\"Learn a vocabulary dictionary of all tokens in the raw documents.\u001b[39;00m\n\u001b[0;32m   1324\u001b[0m \n\u001b[0;32m   1325\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1336\u001b[0m \u001b[38;5;124;03m        Fitted vectorizer.\u001b[39;00m\n\u001b[0;32m   1337\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1338\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1339\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1387\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1379\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1380\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1381\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1382\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1383\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1384\u001b[0m             )\n\u001b[0;32m   1385\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m-> 1387\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[0;32m   1390\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1293\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1291\u001b[0m     vocabulary \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(vocabulary)\n\u001b[0;32m   1292\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m vocabulary:\n\u001b[1;32m-> 1293\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1294\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mempty vocabulary; perhaps the documents only contain stop words\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1295\u001b[0m         )\n\u001b[0;32m   1297\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m indptr[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39miinfo(np\u001b[38;5;241m.\u001b[39mint32)\u001b[38;5;241m.\u001b[39mmax:  \u001b[38;5;66;03m# = 2**31 - 1\u001b[39;00m\n\u001b[0;32m   1298\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _IS_32BIT:\n",
      "\u001b[1;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "# векторизуем текст субтитров обучающей и тестовой выборки\n",
    "vect = CountVectorizer(min_df=4).fit(X_train['subs'])\n",
    "X_train_vect = vect.transform(X_train['subs'])\n",
    "X_test_vect = vect.transform(X_test['subs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4d996356",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vect' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# выведем результаты (признаки) векторизации данных\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m feature_names \u001b[38;5;241m=\u001b[39m \u001b[43mvect\u001b[49m\u001b[38;5;241m.\u001b[39mget_feature_names_out()\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mКоличество признаков: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mlen\u001b[39m(feature_names)))\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mПервые 20 признаков:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(feature_names[:\u001b[38;5;241m20\u001b[39m]))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'vect' is not defined"
     ]
    }
   ],
   "source": [
    "# выведем результаты (признаки) векторизации данных\n",
    "feature_names = vect.get_feature_names_out()\n",
    "print('Количество признаков: {}'.format(len(feature_names)))\n",
    "print('Первые 20 признаков:\\n{}'.format(feature_names[:20]))\n",
    "print('Признаки с 2000 по 2060:\\n{}'.format(feature_names[2000:2060]))\n",
    "print('Каждый 100-й признак:\\n{}'.format(feature_names[::100]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6953a3",
   "metadata": {},
   "source": [
    "## Обучение модели MultinomialNB\n",
    "\n",
    "Для обучения модели на текстовых данных будет использована модель нативного байесовского классификатора MultinomialNB. Такая модель хорошо подходит для высокоразмерных данных, обучается довольно быстро при назначительно меньшем качестве в отличии от линейной модели.\n",
    "\n",
    "Во избежание утечки данных будет использован Pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9a3b11",
   "metadata": {},
   "source": [
    "## Выбор метрики качества\n",
    "\n",
    "В связи с тем, что:\n",
    "\n",
    "<p>в данных обнаружен дисбаланс классов;</p>\n",
    "<p>в текущей задаче является важным как точность ('precision') так и полнота ('recall') предсказания модели;</p>\n",
    "<p>лучшей метрикой для подгонки и оценки модели принимается средне-гармоническое точности и полноты - F1-мера.</p>\n",
    "<p>Для мультиклассификации будет использована взвешенная F1-мера (F1_weighted).</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5355497c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# напишем функцию для построения матрицы ошибок\n",
    "def plot_confusion_matrix(y_test, y_preds, model):\n",
    "    fig, ax = plt.subplots(figsize=(16,10))\n",
    "    cm = confusion_matrix(y_test, y_preds)\n",
    "    cmp = ConfusionMatrixDisplay(cm, display_labels = model.classes_ )\n",
    "    cmp.plot(ax=ax)\n",
    "    plt.suptitle('Матрица ошибок', y=0.92)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fa870c",
   "metadata": {},
   "source": [
    "## Построение pipiline и обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "699ee560",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "\nAll the 750 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n750 fits failed with the following error:\nTraceback (most recent call last):\n  File \"C:\\Users\\Lariosik\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\Lariosik\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 401, in fit\n    Xt = self._fit(X, y, **fit_params_steps)\n  File \"C:\\Users\\Lariosik\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 359, in _fit\n    X, fitted_transformer = fit_transform_one_cached(\n  File \"C:\\Users\\Lariosik\\anaconda3\\lib\\site-packages\\joblib\\memory.py\", line 349, in __call__\n    return self.func(*args, **kwargs)\n  File \"C:\\Users\\Lariosik\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 893, in _fit_transform_one\n    res = transformer.fit_transform(X, y, **fit_params)\n  File \"C:\\Users\\Lariosik\\anaconda3\\lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 142, in wrapped\n    data_to_wrap = f(self, X, *args, **kwargs)\n  File \"C:\\Users\\Lariosik\\anaconda3\\lib\\site-packages\\sklearn\\compose\\_column_transformer.py\", line 727, in fit_transform\n    result = self._fit_transform(X, y, _fit_transform_one)\n  File \"C:\\Users\\Lariosik\\anaconda3\\lib\\site-packages\\sklearn\\compose\\_column_transformer.py\", line 658, in _fit_transform\n    return Parallel(n_jobs=self.n_jobs)(\n  File \"C:\\Users\\Lariosik\\anaconda3\\lib\\site-packages\\sklearn\\utils\\parallel.py\", line 63, in __call__\n    return super().__call__(iterable_with_config)\n  File \"C:\\Users\\Lariosik\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 1051, in __call__\n    while self.dispatch_one_batch(iterator):\n  File \"C:\\Users\\Lariosik\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 864, in dispatch_one_batch\n    self._dispatch(tasks)\n  File \"C:\\Users\\Lariosik\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 782, in _dispatch\n    job = self._backend.apply_async(batch, callback=cb)\n  File \"C:\\Users\\Lariosik\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n    result = ImmediateResult(func)\n  File \"C:\\Users\\Lariosik\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 572, in __init__\n    self.results = batch()\n  File \"C:\\Users\\Lariosik\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 263, in __call__\n    return [func(*args, **kwargs)\n  File \"C:\\Users\\Lariosik\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 263, in <listcomp>\n    return [func(*args, **kwargs)\n  File \"C:\\Users\\Lariosik\\anaconda3\\lib\\site-packages\\sklearn\\utils\\parallel.py\", line 123, in __call__\n    return self.function(*args, **kwargs)\n  File \"C:\\Users\\Lariosik\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 893, in _fit_transform_one\n    res = transformer.fit_transform(X, y, **fit_params)\n  File \"C:\\Users\\Lariosik\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1387, in fit_transform\n    vocabulary, X = self._count_vocab(raw_documents, self.fixed_vocabulary_)\n  File \"C:\\Users\\Lariosik\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1293, in _count_vocab\n    raise ValueError(\nValueError: empty vocabulary; perhaps the documents only contain stop words\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32m<timed exec>:23\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:874\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    868\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m    869\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    870\u001b[0m     )\n\u001b[0;32m    872\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m--> 874\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    878\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1388\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1386\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1387\u001b[0m     \u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1388\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:851\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    844\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m!=\u001b[39m n_candidates \u001b[38;5;241m*\u001b[39m n_splits:\n\u001b[0;32m    845\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    846\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcv.split and cv.get_n_splits returned \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    847\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minconsistent results. Expected \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    848\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplits, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(n_splits, \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m n_candidates)\n\u001b[0;32m    849\u001b[0m     )\n\u001b[1;32m--> 851\u001b[0m \u001b[43m_warn_or_raise_about_fit_failures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    853\u001b[0m \u001b[38;5;66;03m# For callable self.scoring, the return type is only know after\u001b[39;00m\n\u001b[0;32m    854\u001b[0m \u001b[38;5;66;03m# calling. If the return type is a dictionary, the error scores\u001b[39;00m\n\u001b[0;32m    855\u001b[0m \u001b[38;5;66;03m# can now be inserted with the correct key. The type checking\u001b[39;00m\n\u001b[0;32m    856\u001b[0m \u001b[38;5;66;03m# of out will be done in `_insert_error_scores`.\u001b[39;00m\n\u001b[0;32m    857\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m callable(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscoring):\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:367\u001b[0m, in \u001b[0;36m_warn_or_raise_about_fit_failures\u001b[1;34m(results, error_score)\u001b[0m\n\u001b[0;32m    360\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_failed_fits \u001b[38;5;241m==\u001b[39m num_fits:\n\u001b[0;32m    361\u001b[0m     all_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    362\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAll the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    363\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt is very likely that your model is misconfigured.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    364\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can try to debug the error by setting error_score=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    365\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    366\u001b[0m     )\n\u001b[1;32m--> 367\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(all_fits_failed_message)\n\u001b[0;32m    369\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    370\u001b[0m     some_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    371\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mnum_failed_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed out of a total of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    372\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe score on these train-test partitions for these parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    376\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    377\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: \nAll the 750 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n750 fits failed with the following error:\nTraceback (most recent call last):\n  File \"C:\\Users\\Lariosik\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\Lariosik\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 401, in fit\n    Xt = self._fit(X, y, **fit_params_steps)\n  File \"C:\\Users\\Lariosik\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 359, in _fit\n    X, fitted_transformer = fit_transform_one_cached(\n  File \"C:\\Users\\Lariosik\\anaconda3\\lib\\site-packages\\joblib\\memory.py\", line 349, in __call__\n    return self.func(*args, **kwargs)\n  File \"C:\\Users\\Lariosik\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 893, in _fit_transform_one\n    res = transformer.fit_transform(X, y, **fit_params)\n  File \"C:\\Users\\Lariosik\\anaconda3\\lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 142, in wrapped\n    data_to_wrap = f(self, X, *args, **kwargs)\n  File \"C:\\Users\\Lariosik\\anaconda3\\lib\\site-packages\\sklearn\\compose\\_column_transformer.py\", line 727, in fit_transform\n    result = self._fit_transform(X, y, _fit_transform_one)\n  File \"C:\\Users\\Lariosik\\anaconda3\\lib\\site-packages\\sklearn\\compose\\_column_transformer.py\", line 658, in _fit_transform\n    return Parallel(n_jobs=self.n_jobs)(\n  File \"C:\\Users\\Lariosik\\anaconda3\\lib\\site-packages\\sklearn\\utils\\parallel.py\", line 63, in __call__\n    return super().__call__(iterable_with_config)\n  File \"C:\\Users\\Lariosik\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 1051, in __call__\n    while self.dispatch_one_batch(iterator):\n  File \"C:\\Users\\Lariosik\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 864, in dispatch_one_batch\n    self._dispatch(tasks)\n  File \"C:\\Users\\Lariosik\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 782, in _dispatch\n    job = self._backend.apply_async(batch, callback=cb)\n  File \"C:\\Users\\Lariosik\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n    result = ImmediateResult(func)\n  File \"C:\\Users\\Lariosik\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 572, in __init__\n    self.results = batch()\n  File \"C:\\Users\\Lariosik\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 263, in __call__\n    return [func(*args, **kwargs)\n  File \"C:\\Users\\Lariosik\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 263, in <listcomp>\n    return [func(*args, **kwargs)\n  File \"C:\\Users\\Lariosik\\anaconda3\\lib\\site-packages\\sklearn\\utils\\parallel.py\", line 123, in __call__\n    return self.function(*args, **kwargs)\n  File \"C:\\Users\\Lariosik\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 893, in _fit_transform_one\n    res = transformer.fit_transform(X, y, **fit_params)\n  File \"C:\\Users\\Lariosik\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1387, in fit_transform\n    vocabulary, X = self._count_vocab(raw_documents, self.fixed_vocabulary_)\n  File \"C:\\Users\\Lariosik\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1293, in _count_vocab\n    raise ValueError(\nValueError: empty vocabulary; perhaps the documents only contain stop words\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# выделим отдельную категорию признаков\n",
    "other_colls = X_train.columns[1:]\n",
    "\n",
    "# инициализируем вектор-объект\n",
    "vector = CountVectorizer(min_df=4)\n",
    "\n",
    "# создадим препроцессор для групп признаков\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('other', 'passthrough', other_colls),\n",
    "    ('txt', vector, 'subs')\n",
    "])\n",
    "\n",
    "# инициализируем модель MultinomialNB\n",
    "classifier = MultinomialNB()\n",
    "\n",
    "# создадим pipeline\n",
    "pipe = Pipeline(steps=[('prep', preprocessor),\n",
    "                       ('clf', classifier)])\n",
    "\n",
    "# обучим модель с подбором гиперпараметра alpha на 5 фолдах\n",
    "param_grid = {'clf__alpha': np.arange(0.001, 0.3, 0.002)}\n",
    "grid = GridSearchCV(pipe, param_grid, cv=5, scoring='f1_weighted')\n",
    "grid.fit(X_train, y_train)\n",
    "# выведем лучший score и значение гиперпараметра\n",
    "print(grid.best_score_)\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b9ba76",
   "metadata": {},
   "source": [
    "Лучшая модель показала результат - 0.658 (F1_weighted) при подобранном гиперпараметре 'alpha' равном 0.137.\n",
    "\n",
    "Оценим модель на тестовой выборке."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1012098",
   "metadata": {},
   "source": [
    "## Оценка модели на тестовой (отложенной) выборке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8d378474",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "This GridSearchCV instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# получим предсказания на тестовой выборке\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mgrid\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# выведем раезльтаты оценки по метрике F1-weighted\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mМетрика F1-weighted на обучающей выборке:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf1_score(grid\u001b[38;5;241m.\u001b[39mpredict(X_train), y_train, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweighted\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:498\u001b[0m, in \u001b[0;36mBaseSearchCV.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    479\u001b[0m \u001b[38;5;129m@available_if\u001b[39m(_estimator_has(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredict\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m    480\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m    481\u001b[0m     \u001b[38;5;124;03m\"\"\"Call predict on the estimator with the best found parameters.\u001b[39;00m\n\u001b[0;32m    482\u001b[0m \n\u001b[0;32m    483\u001b[0m \u001b[38;5;124;03m    Only available if ``refit=True`` and the underlying estimator supports\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    496\u001b[0m \u001b[38;5;124;03m        the best found parameters.\u001b[39;00m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 498\u001b[0m     \u001b[43mcheck_is_fitted\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    499\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbest_estimator_\u001b[38;5;241m.\u001b[39mpredict(X)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1390\u001b[0m, in \u001b[0;36mcheck_is_fitted\u001b[1;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[0;32m   1385\u001b[0m     fitted \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m   1386\u001b[0m         v \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mvars\u001b[39m(estimator) \u001b[38;5;28;01mif\u001b[39;00m v\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m v\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1387\u001b[0m     ]\n\u001b[0;32m   1389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fitted:\n\u001b[1;32m-> 1390\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NotFittedError(msg \u001b[38;5;241m%\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(estimator)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m})\n",
      "\u001b[1;31mNotFittedError\u001b[0m: This GridSearchCV instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator."
     ]
    }
   ],
   "source": [
    "# получим предсказания на тестовой выборке\n",
    "predictions = grid.predict(X_test)\n",
    "# выведем раезльтаты оценки по метрике F1-weighted\n",
    "print(f'Метрика F1-weighted на обучающей выборке:{f1_score(grid.predict(X_train), y_train, average=\"weighted\")}')\n",
    "print(f'Метрика F1-weighted на тестовой выборке:{f1_score(predictions, y_test, average=\"weighted\")}')\n",
    "print('-'*50)\n",
    "# выведем классификационную таблицу и матрицу ошибок\n",
    "print(classification_report(y_test, predictions))\n",
    "plot_confusion_matrix(y_test, predictions, grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c85fb4a",
   "metadata": {},
   "source": [
    "Лучшая модель показала результат - 0.63 (F1_weighted) на тестовой выборке при подобранном гиперпараметре 'alpha' равном 0.137."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24179bd3",
   "metadata": {},
   "source": [
    "## Выводы по результатам работы модели"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447bcd63",
   "metadata": {},
   "source": [
    "Из оценки классификатора на тестовой выборке можно сделать следующие выводы:\n",
    "\n",
    "как и ожидалось, модель чаще всего ошибается на граничных метках. Например модель часто назначает фильмам с меткой B2 метку B1. По условиям задачи где, студенты должны минимум понимать 50-70% диалогов, ошибка прогнозирования в сторону наименьшего класса допустима в пределах граничных классов;\n",
    "хуже всего классифицированы фильмы с метками A1 и B1. В первом случае за счет низкого значения метрики recall (среди небольшого количества фильмов представленных в этом классе меток (7 фильмов), всего 2 фильма размечены верно). Во втором случае за счет низкого значения метрики precision (модель часто ошибается в присвоении метки B1, т.е. среди всех размеченных фильмов с присвоением метки B1 (21 фильм) только 8 размечены верно.\n",
    "Использование дополнительно созданных признаков, с количеством уникальных слов из классического словаря Oxford в каждом фильме, не дали значительного прироста в качестве предсказаний, что может подтверждать предположение о субъективности разметки представленных Заказчиком фильмов, в силу предвзятости специалистов по английскому языку в отношении сложности диалогов.\n",
    "\n",
    "В целом с учетом предположения о субъективности разметки представленных Заказчиком фильмов, а так же использования быстрого и простого наивного Байесовского классификатора, удалось достичь приемлемых результатов. Взвешенная метрика F1-weighted на тестовой выборке составила 0.63."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d832cb36",
   "metadata": {},
   "source": [
    "## Обучение модели MultinomialNB на всех данных. Сохранение модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8090b533",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "\nAll the 750 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n750 fits failed with the following error:\nTraceback (most recent call last):\n  File \"C:\\Users\\Lariosik\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\Lariosik\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 401, in fit\n    Xt = self._fit(X, y, **fit_params_steps)\n  File \"C:\\Users\\Lariosik\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 359, in _fit\n    X, fitted_transformer = fit_transform_one_cached(\n  File \"C:\\Users\\Lariosik\\anaconda3\\lib\\site-packages\\joblib\\memory.py\", line 349, in __call__\n    return self.func(*args, **kwargs)\n  File \"C:\\Users\\Lariosik\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 893, in _fit_transform_one\n    res = transformer.fit_transform(X, y, **fit_params)\n  File \"C:\\Users\\Lariosik\\anaconda3\\lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 142, in wrapped\n    data_to_wrap = f(self, X, *args, **kwargs)\n  File \"C:\\Users\\Lariosik\\anaconda3\\lib\\site-packages\\sklearn\\compose\\_column_transformer.py\", line 727, in fit_transform\n    result = self._fit_transform(X, y, _fit_transform_one)\n  File \"C:\\Users\\Lariosik\\anaconda3\\lib\\site-packages\\sklearn\\compose\\_column_transformer.py\", line 658, in _fit_transform\n    return Parallel(n_jobs=self.n_jobs)(\n  File \"C:\\Users\\Lariosik\\anaconda3\\lib\\site-packages\\sklearn\\utils\\parallel.py\", line 63, in __call__\n    return super().__call__(iterable_with_config)\n  File \"C:\\Users\\Lariosik\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 1051, in __call__\n    while self.dispatch_one_batch(iterator):\n  File \"C:\\Users\\Lariosik\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 864, in dispatch_one_batch\n    self._dispatch(tasks)\n  File \"C:\\Users\\Lariosik\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 782, in _dispatch\n    job = self._backend.apply_async(batch, callback=cb)\n  File \"C:\\Users\\Lariosik\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n    result = ImmediateResult(func)\n  File \"C:\\Users\\Lariosik\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 572, in __init__\n    self.results = batch()\n  File \"C:\\Users\\Lariosik\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 263, in __call__\n    return [func(*args, **kwargs)\n  File \"C:\\Users\\Lariosik\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 263, in <listcomp>\n    return [func(*args, **kwargs)\n  File \"C:\\Users\\Lariosik\\anaconda3\\lib\\site-packages\\sklearn\\utils\\parallel.py\", line 123, in __call__\n    return self.function(*args, **kwargs)\n  File \"C:\\Users\\Lariosik\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 893, in _fit_transform_one\n    res = transformer.fit_transform(X, y, **fit_params)\n  File \"C:\\Users\\Lariosik\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1387, in fit_transform\n    vocabulary, X = self._count_vocab(raw_documents, self.fixed_vocabulary_)\n  File \"C:\\Users\\Lariosik\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1293, in _count_vocab\n    raise ValueError(\nValueError: empty vocabulary; perhaps the documents only contain stop words\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m param_grid \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclf__alpha\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m0.001\u001b[39m, \u001b[38;5;241m0.3\u001b[39m, \u001b[38;5;241m0.002\u001b[39m)}\n\u001b[0;32m      6\u001b[0m grid \u001b[38;5;241m=\u001b[39m GridSearchCV(pipe, param_grid, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf1_weighted\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 7\u001b[0m \u001b[43mgrid\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_full\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_full\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# выведем лучший score и значение гиперпараметра\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(grid\u001b[38;5;241m.\u001b[39mbest_score_)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:874\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    868\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m    869\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    870\u001b[0m     )\n\u001b[0;32m    872\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m--> 874\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    878\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1388\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1386\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1387\u001b[0m     \u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1388\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:851\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    844\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m!=\u001b[39m n_candidates \u001b[38;5;241m*\u001b[39m n_splits:\n\u001b[0;32m    845\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    846\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcv.split and cv.get_n_splits returned \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    847\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minconsistent results. Expected \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    848\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplits, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(n_splits, \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m n_candidates)\n\u001b[0;32m    849\u001b[0m     )\n\u001b[1;32m--> 851\u001b[0m \u001b[43m_warn_or_raise_about_fit_failures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    853\u001b[0m \u001b[38;5;66;03m# For callable self.scoring, the return type is only know after\u001b[39;00m\n\u001b[0;32m    854\u001b[0m \u001b[38;5;66;03m# calling. If the return type is a dictionary, the error scores\u001b[39;00m\n\u001b[0;32m    855\u001b[0m \u001b[38;5;66;03m# can now be inserted with the correct key. The type checking\u001b[39;00m\n\u001b[0;32m    856\u001b[0m \u001b[38;5;66;03m# of out will be done in `_insert_error_scores`.\u001b[39;00m\n\u001b[0;32m    857\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m callable(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscoring):\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:367\u001b[0m, in \u001b[0;36m_warn_or_raise_about_fit_failures\u001b[1;34m(results, error_score)\u001b[0m\n\u001b[0;32m    360\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_failed_fits \u001b[38;5;241m==\u001b[39m num_fits:\n\u001b[0;32m    361\u001b[0m     all_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    362\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAll the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    363\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt is very likely that your model is misconfigured.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    364\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can try to debug the error by setting error_score=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    365\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    366\u001b[0m     )\n\u001b[1;32m--> 367\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(all_fits_failed_message)\n\u001b[0;32m    369\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    370\u001b[0m     some_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    371\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mnum_failed_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed out of a total of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    372\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe score on these train-test partitions for these parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    376\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    377\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: \nAll the 750 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n750 fits failed with the following error:\nTraceback (most recent call last):\n  File \"C:\\Users\\Lariosik\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\Lariosik\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 401, in fit\n    Xt = self._fit(X, y, **fit_params_steps)\n  File \"C:\\Users\\Lariosik\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 359, in _fit\n    X, fitted_transformer = fit_transform_one_cached(\n  File \"C:\\Users\\Lariosik\\anaconda3\\lib\\site-packages\\joblib\\memory.py\", line 349, in __call__\n    return self.func(*args, **kwargs)\n  File \"C:\\Users\\Lariosik\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 893, in _fit_transform_one\n    res = transformer.fit_transform(X, y, **fit_params)\n  File \"C:\\Users\\Lariosik\\anaconda3\\lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 142, in wrapped\n    data_to_wrap = f(self, X, *args, **kwargs)\n  File \"C:\\Users\\Lariosik\\anaconda3\\lib\\site-packages\\sklearn\\compose\\_column_transformer.py\", line 727, in fit_transform\n    result = self._fit_transform(X, y, _fit_transform_one)\n  File \"C:\\Users\\Lariosik\\anaconda3\\lib\\site-packages\\sklearn\\compose\\_column_transformer.py\", line 658, in _fit_transform\n    return Parallel(n_jobs=self.n_jobs)(\n  File \"C:\\Users\\Lariosik\\anaconda3\\lib\\site-packages\\sklearn\\utils\\parallel.py\", line 63, in __call__\n    return super().__call__(iterable_with_config)\n  File \"C:\\Users\\Lariosik\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 1051, in __call__\n    while self.dispatch_one_batch(iterator):\n  File \"C:\\Users\\Lariosik\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 864, in dispatch_one_batch\n    self._dispatch(tasks)\n  File \"C:\\Users\\Lariosik\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 782, in _dispatch\n    job = self._backend.apply_async(batch, callback=cb)\n  File \"C:\\Users\\Lariosik\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n    result = ImmediateResult(func)\n  File \"C:\\Users\\Lariosik\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 572, in __init__\n    self.results = batch()\n  File \"C:\\Users\\Lariosik\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 263, in __call__\n    return [func(*args, **kwargs)\n  File \"C:\\Users\\Lariosik\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 263, in <listcomp>\n    return [func(*args, **kwargs)\n  File \"C:\\Users\\Lariosik\\anaconda3\\lib\\site-packages\\sklearn\\utils\\parallel.py\", line 123, in __call__\n    return self.function(*args, **kwargs)\n  File \"C:\\Users\\Lariosik\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 893, in _fit_transform_one\n    res = transformer.fit_transform(X, y, **fit_params)\n  File \"C:\\Users\\Lariosik\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1387, in fit_transform\n    vocabulary, X = self._count_vocab(raw_documents, self.fixed_vocabulary_)\n  File \"C:\\Users\\Lariosik\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1293, in _count_vocab\n    raise ValueError(\nValueError: empty vocabulary; perhaps the documents only contain stop words\n"
     ]
    }
   ],
   "source": [
    "# обучим модель с подбором гиперпараметра alpha на 5 фолдах\n",
    "# на всей выборке и оценим качество\n",
    "X_full = pd.concat([X_train, X_test])\n",
    "y_full = pd.concat([y_train, y_test])\n",
    "param_grid = {'clf__alpha': np.arange(0.001, 0.3, 0.002)}\n",
    "grid = GridSearchCV(pipe, param_grid, cv=5, scoring='f1_weighted')\n",
    "grid.fit(X_full, y_full)\n",
    "# выведем лучший score и значение гиперпараметра\n",
    "print(grid.best_score_)\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "38aa8818",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dump' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# сохраним модель в папку models\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mdump\u001b[49m(grid, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD:/English_films_level/models/model_bayesNB.joblib\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# сохраним классический oxford словарь\u001b[39;00m\n\u001b[0;32m      4\u001b[0m dump(oxford\u001b[38;5;241m.\u001b[39mdata, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD:/English_films_level/oxfordclassic_oxford.joblib\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dump' is not defined"
     ]
    }
   ],
   "source": [
    "# сохраним модель в папку models\n",
    "dump(grid, 'D:/English_films_level/models/model_bayesNB.joblib')\n",
    "# сохраним классический oxford словарь\n",
    "dump(oxford.data, 'D:/English_films_level/oxfordclassic_oxford.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1724c976",
   "metadata": {},
   "source": [
    "## Выводы и рекомендации"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ef897c",
   "metadata": {},
   "source": [
    "При решении задачи обнаружены следующие проблеммы:\n",
    "\n",
    "Отсутствуют фильмы с уровнем сложности A1;\n",
    "\n",
    "В данных присутствует сильный дисбаланс по остальным классам (меткам):\n",
    "37 фильмов с уровнем сложности 3 (метка B2); 39 фильмов с уровнем сложности 2 (метка B1); 6 фильмов с уровнем сложности 4 (метка C1); 25 фильма с уровнем сложности 1 (метка A2).\n",
    "\n",
    "Наличие большой вероятности в субъективности разметки данных в фильмах. Каждый специалист может немного по-разному определять уровень сложности понимания английского. В таком случае можно ожидать смещения в предсказании любой модели, так как субъективность часто вносит неустранимую ошибку в предиктивность модели. С большой долей вероятности модель будет допускать ошибки в предсказании. Особенно на граничных значениях меток (например ошибаться в присвоении метки A2 или B1, B1 или B2). Такие ошибки впринципе допустимы в отношении поставленной задачи, так как позволяют добиться того, чтобы студент понимал 50-70 % диалогов даже при ошибочном предсказании модели в сторону присвоения более низкой метки, так как различия между такими классами менее существенны.\n",
    "\n",
    "Выводы по результатам работы модели:\n",
    "\n",
    "Лучшая модель показала результат - 0.63 (F1_weighted) на тестовой выборке при подобранном гиперпараметре 'alpha' равном 0.137.\n",
    "\n",
    "как и ожидалось, модель чаще всего ошибается на граничных метках. Например модель часто назначает фильмам с меткой B2 метку B1. По условиям задачи где, студенты должны минимум понимать 50-70% диалогов, ошибка прогнозирования в сторону наименьшего класса допустима в пределах граничных классов;\n",
    "хуже всего классифицированы фильмы с метками A1 и B1. В первом случае за счет низкого значения метрики recall (среди небольшого количества фильмов представленных в этом классе меток (7 фильмов), всего 2 фильма размечены верно). Во втором случае за счет низкого значения метрики precision (модель часто ошибается в присвоении метки B1, т.е. среди всех размеченных фильмов с присвоением метки B1 (21 фильм) только 8 размечены верно.\n",
    "\n",
    "Использование дополнительно созданных признаков, с количеством уникальных слов из классического словаря Oxford в каждом фильме, не дали значительного прироста в качестве предсказаний, что может подтверждать предположение о субъективности разметки представленных Заказчиком фильмов, в силу предвзятости специалистов по английскому языку в отношении сложности диалогов.\n",
    "\n",
    "В целом с учетом предположения о субъективности разметки представленных Заказчиком фильмов, а так же использования быстрого и простого наивного Байесовского классификатора, удалось достичь приемлемых результатов. Взвешенная метрика F1-weighted на модели, обученной на всех представленных данных, при кроссвалидации на 5-и фолдах показала результат в 0.68 по метрике F1-weighted.\n",
    "\n",
    "Рекомендации:\n",
    "\n",
    "Собрать больше фильмов с размеченным уровнем сложности с одинаковым количеством в каждом классе, включая уровень A1. В данном случае можно добиться лучшей пргоностической способности модели;\n",
    "Разметка фильмов по уровню сложности должна быть лишена субъективности и опираться на общепринятые правила определения. Размеченные данные (фильмы) должны поступать от классифицированных специалистов по английскому языку, либо с общеизвесиных и общепринятых источников."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8265c5cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
